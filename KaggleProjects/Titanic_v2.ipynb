{
  "cells": [
    {
      "metadata": {
        "_uuid": "d21e85c61e5ffe15ed891976c8ef195fa476ba2c"
      },
      "cell_type": "markdown",
      "source": "**This is my second Kernel on Titanic Survival Challenge. The purpose of this kernel is to try feature engineering and see if it can improve the score. Also, I have been reading different kernels on this problem so, I though it is good to put some of the best practices people are using in their kernels.** \n\n***In Short, learning by doing!!***"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#import libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#We will use different classifiers and then try Voting Classifier to see if it helps in increasing score.\n\n#Classfiers used in _v1\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n#Ensemble\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n\n#Model_selection\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n#set data visualization stying\n\nsns.set(style='white', context = 'notebook', palette='deep')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb8eb0c567ed7e62582ea9dc9cccf1190cf8e867"
      },
      "cell_type": "code",
      "source": "#Load data\n\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\nIDtest = test_data[\"PassengerId\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "10466b445dc50f462127914b9ddf6a9cec931bef"
      },
      "cell_type": "markdown",
      "source": "While doing an online course, that we should remove outliers, if possible, from the data. The method they used was find 1st and 3rd quartiles and get **interquartile ranges** (IQR), $Q3-Q1$ and see if there are data points which are beyond $1.5*IQR$ on both sides. \n\nThen if any row which has more than 2 or more outlier columns, can be removed from training data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b61674d2714e702a219b5629dfd850622c041abe"
      },
      "cell_type": "code",
      "source": "#Outlier Detection\n\nfrom collections import Counter\n\ndef get_outliers(df,n,features):\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        #1st Quartile\n        Q1 = np.percentile(df[col],25)\n        #3rd Quartile\n        Q3 = np.percentile(df[col],75)\n        #Inter-Quartile Range\n        IQR = Q3 - Q1\n        \n        #Outliers Range        \n        outliers_boundary = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        list_outlier_cols = df[(df[col] < Q1 - outliers_boundary) | (df[col] > Q3 + outliers_boundary)].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(list_outlier_cols)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    more_than_two_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return more_than_two_outliers   \n\n#Function takes 3 parameters - DataFrame,number of outliers you want to check in an observations, feature columns \nOutliers_to_drop = get_outliers(train_data, 2, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "377f475512ebee543e38d28a49ad5eb611d18c42"
      },
      "cell_type": "code",
      "source": "train_data.loc[Outliers_to_drop]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0b4af173e22cb047f1b299608cd59f83c1ddc3f"
      },
      "cell_type": "code",
      "source": "#Drop outliers\n\ntrain_data = train_data.drop(Outliers_to_drop, axis = 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c8f36823b0fd25b1b257ba53b1097e34b9122fe"
      },
      "cell_type": "markdown",
      "source": "Out of 10 outliers, 3 have very high ticket fares as compared to other, and 7 have high value of SibSp"
    },
    {
      "metadata": {
        "_uuid": "c1894501d094b7ca5bed7171090eee192d472742"
      },
      "cell_type": "markdown",
      "source": "In this Titanic_v2, I will combine the test and training data and then we will do all the operations on both the data set. This can be done by creating data_processing function and then call over two data sets insdividually as well, which was the idea in Titanic_v1 ( although we didn't create a function then and ran the steps individaully twice. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6b04708bcab6cf3f779aa8435e466d5ce60628c"
      },
      "cell_type": "code",
      "source": "#Concate datasets\nfull_dataset = pd.concat(objs=[train_data,test_data], axis = 0).reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abae2b203a5bd004a449b8da6ebeb9bfa8a17c5f"
      },
      "cell_type": "code",
      "source": "full_dataset.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b098f1c566625013a063c01e0ae0202eabad3c50"
      },
      "cell_type": "code",
      "source": "full_dataset.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5558410d107bdaa22682b48c942fbc750c1e6a11"
      },
      "cell_type": "markdown",
      "source": "Survived has 418 NaN because the dataset is a combination of test and train data and test data doesn't contain Survived columns."
    },
    {
      "metadata": {
        "_uuid": "18a02fd54ec9bbadd55bb8129ea6162f90a18074"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering"
    },
    {
      "metadata": {
        "_uuid": "26a4af1cad8dc78f808523231b76882b63cc9a27"
      },
      "cell_type": "markdown",
      "source": "\nWe will analyse train data set features but whatever operation we will be doing on data, it will be on combined dataset (final_dataset)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "495f9ab0a40a98991227fa5128d73190e3e23d28"
      },
      "cell_type": "code",
      "source": "#In Version V1, we didn't look into SibSp and Parch features, so let's start with them.\n\ntrain_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "29baeb83fa17451d3de276e5f0baa446ee7e4a7d"
      },
      "cell_type": "markdown",
      "source": "Interesting, passengers with less numbers of siblings tends to survive more. For example 0, 1 and 2 sibling passengers have 34%, 46% and 53% survival rate. \n\nThis can be included in our new features."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c42a351bb986c516597a6ab66a13f5ed9a4654b0"
      },
      "cell_type": "code",
      "source": "#Let's do the similar analysis for Parch\n\ntrain_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f5ac6b365772e1003ae34104152feda52156b458"
      },
      "cell_type": "markdown",
      "source": "Again, small families have more chnaces to survive 3 being highest."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9754a067e7203b1631f97a4246e0c41da8d57d64"
      },
      "cell_type": "code",
      "source": "#As we can see large family have less survival rate, I am going to make a new feature which we can call family size.\n#Family size = SibSp + Parch + Individual\n\nfull_dataset[\"FamilySize\"] = full_dataset[\"SibSp\"] + full_dataset[\"Parch\"] + 1\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "711e82c0e594aab8973c89cfd021404e6ad4bce7"
      },
      "cell_type": "code",
      "source": "full_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11bf2808d4cdb8f58dc10e77bd7e2addf6d80b6c"
      },
      "cell_type": "code",
      "source": "#Also I am going to drop Cabin Variable as Cabin has more than 70% null\n\nfull_dataset.drop([\"Cabin\"],axis =1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64c06585747ad1ee20542c22468d13f9cd177dd7"
      },
      "cell_type": "code",
      "source": "full_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1e6e49fb65369b0b239668daa4df83f5a198a9a"
      },
      "cell_type": "markdown",
      "source": "Last time we imputed age with median age of the population, but this time we are going to do something extra. I will find the correlation of age with different features and see if age can be imputed on the sub-population level.\n\n"
    },
    {
      "metadata": {
        "_uuid": "24562054b8399ed4690d3030fc11c97ca0a38ee9"
      },
      "cell_type": "markdown",
      "source": "# Fill missing Values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7dbd679e5b5e3f80b28360651678f12d0fede6b"
      },
      "cell_type": "code",
      "source": "#Embarked - Since only two values are missing and that is in training data, we can fill it with highest occuring value as we did in V1.\nfull_dataset[\"Embarked\"].fillna('S', inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5565e40ae718fd43088253cc27b9e3b8b760ab6f"
      },
      "cell_type": "code",
      "source": "#Fare can be filled with median value as well.\n\nfare_median = full_dataset[\"Fare\"].median()\nfare_median\n\nfull_dataset[\"Fare\"].fillna(fare_median, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ef2c1431a2e966b8b9245fba38c900c92f2bc83"
      },
      "cell_type": "code",
      "source": "#Let's do one hot encoding like V1.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle_pClass = LabelEncoder()\nle_sex = LabelEncoder()\nle_embarked = LabelEncoder()\nfull_dataset['PClass_encoded'] = le_pClass.fit_transform(full_dataset.Pclass)\nfull_dataset['Sex_encoded'] = le_sex.fit_transform(full_dataset.Sex)\nfull_dataset['Embarked_encoded'] = le_embarked.fit_transform(full_dataset.Embarked)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05206866030cbbdddded0d2e13e10e5b01b304f5"
      },
      "cell_type": "code",
      "source": "full_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e5e76b3c1248ccc0f52841b1cbeb81fd645dd03"
      },
      "cell_type": "code",
      "source": "#One hot encoding for categorical columns (PClass, Sex, Embarked)\n\nfrom sklearn.preprocessing import OneHotEncoder\n\npClass_ohe = OneHotEncoder()\nsex_ohe = OneHotEncoder()\nembarked_ohe = OneHotEncoder()\n\nXp =pClass_ohe.fit_transform(full_dataset.PClass_encoded.values.reshape(-1,1)).toarray()\nXs =sex_ohe.fit_transform(full_dataset.Sex_encoded.values.reshape(-1,1)).toarray()\nXe =embarked_ohe.fit_transform(full_dataset.Embarked_encoded.values.reshape(-1,1)).toarray()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f22a562479840e9657c870c0fb042e548fdedcd6"
      },
      "cell_type": "code",
      "source": "#Add back to original dataframe\n\ntrain_dataOneHot = pd.DataFrame(Xp, columns = [\"PClass_\"+str(int(i)) for i in range(Xp.shape[1])])\nfull_dataset = pd.concat([full_dataset, train_dataOneHot], axis=1)\n\ntrain_dataOneHot = pd.DataFrame(Xs, columns = [\"Sex_\"+str(int(i)) for i in range(Xs.shape[1])])\nfull_dataset = pd.concat([full_dataset, train_dataOneHot], axis=1)\n\ntrain_dataOneHot = pd.DataFrame(Xe, columns = [\"Embarked_\"+str(int(i)) for i in range(Xe.shape[1])])\nfull_dataset = pd.concat([full_dataset, train_dataOneHot], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5311d8e4240b5a22fda090c57886a63d3c9ab1a3"
      },
      "cell_type": "code",
      "source": "full_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a4b923861d38bc69abdf2a97d714d0ad59647fb"
      },
      "cell_type": "markdown",
      "source": "Last time we didn't do anything with two features, Name and Ticket. Also, we imputed Age with median. let's see if we can do something more this time."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "507f28f8ea03850f80fa530fe3cf596acd610cf9"
      },
      "cell_type": "code",
      "source": "#First, let us take age.\n#Let us see how other features are correlated with age and if we can impute age as per other features.\n\ng = sns.catplot(y=\"Age\",x=\"Sex\",data=full_dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=full_dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Parch\", data=full_dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"SibSp\", data=full_dataset,kind=\"box\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86c86cff5b6ee2664d95e6fe72ed8b73114dc8f0"
      },
      "cell_type": "code",
      "source": "#Convert Sex feature into 0 and 1 and then check correlation matrix.\n\nfull_dataset[\"Sex\"] = full_dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n\ng = sns.heatmap(full_dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e33cc4e26c87bedfef0e00670b2966447e15729"
      },
      "cell_type": "markdown",
      "source": "Factorplot and correlation matrix tells us that while age is not related to Sex of the passenger but it is negatively correlated to SibSp, Parch and PClass, so we can impute the age of the passenger, where it is not present , with the median of age of similar rows of PClass, SibSp, and Parch."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "437cb0f8693f7443a0c177deca045b7c394a3540"
      },
      "cell_type": "code",
      "source": "#Get indexes of rows with NaN as age.\n#We are getting indexes of all the columns and then getting back all the indexes where Age is null\n\nindex_NaN_age = list(full_dataset[\"Age\"][full_dataset[\"Age\"].isnull()].index)\n\n\nfor i in index_NaN_age:\n    age_med = full_dataset[\"Age\"].median()\n    age_pred = full_dataset[\"Age\"][((full_dataset['SibSp'] == full_dataset.iloc[i][\"SibSp\"]) \n                                    & (full_dataset['Parch'] == full_dataset.iloc[i][\"Parch\"]) \n                                    & (full_dataset['Pclass'] == full_dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        full_dataset['Age'].iloc[i] = age_pred\n    else :\n        full_dataset['Age'].iloc[i] = age_med",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4106a3b3b0630fa7b154fc1894c7365bd00c241"
      },
      "cell_type": "markdown",
      "source": "We have no Nan in our dataset now, but we have not done feature engineering on Ticket and Name feature yet. Can we do something there?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df231585348280ad385aac2527807b8fd0efe633"
      },
      "cell_type": "code",
      "source": "#Name, Get title from the name.\n\nfull_dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in full_dataset[\"Name\"]]\nfull_dataset[\"Title\"] = pd.Series(full_dataset_title)\nfull_dataset[\"Title\"].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d0ef1e916676836026c70536fc8c169539ffb54"
      },
      "cell_type": "code",
      "source": "#Histogram for Titles\n\ng = sns.countplot(x=\"Title\", data = full_dataset)\ng = plt.setp(g.get_xticklabels(), rotation = 45)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e21ed2063c5898056bf22d89fb59fc62cad8ee9"
      },
      "cell_type": "markdown",
      "source": "There are mainly 4 titles and all other titles are very rare. We will make 4 titles, Mr, Mrs/Miss, Master, Others"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "839724fe5d3f4c626d31784489e282c71898f0a7"
      },
      "cell_type": "code",
      "source": "\n#Replace with Rare\nfull_dataset[\"Title\"] = full_dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', \n                                             'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n#Replace feminine titles with Ms.\nfull_dataset[\"Title\"] = full_dataset[\"Title\"].replace(['Miss', 'Ms','Mme','Mlle', 'Mrs'], 'Ms')\n\n#Map titles\nfull_dataset[\"Title\"] = full_dataset[\"Title\"].map({\"Master\":0, \"Ms\":1 ,\"Mr\":2, \"Rare\":3})\nfull_dataset[\"Title\"] = full_dataset[\"Title\"].astype(int)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1217bd7e355132a5f84e2357e45877604f26ede2"
      },
      "cell_type": "code",
      "source": "#Histogram Again\ng = sns.countplot(x=\"Title\", data = full_dataset)\ng = g.set_xticklabels([\"Master\",\"Ms\",\"Mr\",\"Rare\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c38b7cd0956efa250bb8c896a62e5a0007c8e8c9"
      },
      "cell_type": "code",
      "source": "#Let us see if survival chnaces depends on titles.\n\ng = sns.catplot(x=\"Title\", y= \"Survived\", data = full_dataset, kind ='bar')\ng = g.set_xticklabels([\"Master\",\"Ms\",\"Mr\",\"Rare\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0076057097654ff645ce3818a4f91a6c6dc9a587"
      },
      "cell_type": "markdown",
      "source": "Clearly women and childrens have higher rate of survivals."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65b5c42bf101aaa99241875f15ef23e43fa60d56"
      },
      "cell_type": "code",
      "source": "#We can now remove Name column\n\nfull_dataset.drop([\"Name\"], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6cc0cceabb28a790fa61441383361e92f9bba3c"
      },
      "cell_type": "markdown",
      "source": "I am going to work on Ticket, Cabin and FamilySize in the next iteration."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9cc9c3415f95b45946c6e68d0598db048928161"
      },
      "cell_type": "code",
      "source": "full_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46973703c3530ebc49130dc12c060bac8f778bc8"
      },
      "cell_type": "markdown",
      "source": "# Modeling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f51514c4c89c9ac3f67fdddc3c1e2bb27764bf38"
      },
      "cell_type": "code",
      "source": "#Drop extra columns\n\nfull_dataset.drop([\"PassengerId\",\"Embarked\",\"Pclass\",\"Sex\", \"Ticket\",\"Parch\", \"SibSp\", \n                   \"PClass_encoded\",\"Sex_encoded\",\"Embarked_encoded\"]\n                , axis =1, inplace=True)\n\nfull_dataset.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f42ebe771a96a2d169f34b04e7820ab9ab876c5"
      },
      "cell_type": "code",
      "source": "train_len = len(train_data)\ntrain_len",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa5af20801837e42c30f5b32e9861862f0240fd9"
      },
      "cell_type": "code",
      "source": "#Let's divide data into Train and test now.\n\ntrain_data = full_dataset[:train_len]\ntest_data = full_dataset[train_len:]\n\n#drop survived column from test_data\n\ntest_data.drop([\"Survived\"], axis =1, inplace=True)\n\nprint(train_data.shape)\nprint(test_data.shape)\n      ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89ae85c59d810e011820580d561ce11143812475"
      },
      "cell_type": "code",
      "source": "#Separating features and target variable from training data\n\ntrain_data[\"Survived\"] = train_data[\"Survived\"].astype(int)\ntrain_data[\"Fare\"] = train_data[\"Fare\"].astype(float)\n\n\ny_train = train_data[\"Survived\"]\nX_train = train_data.drop(labels=[\"Survived\"], axis =1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4172705b8ec6b7f2826869413dab3c5abe2bcaf7"
      },
      "cell_type": "markdown",
      "source": "In this version 2 I will be using:\n\n* SVC\n* RandomForest\n* AdaBoost\n* Decision Tree\n* Extra Trees\n* Gradient Boosting\n* KNN\n* Logistic Regression\n\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c05b959aef47e03e31f00b1f687a37d5bf4941b4"
      },
      "cell_type": "code",
      "source": "#10 fold cross validation\n\nkfold = StratifiedKFold(n_splits=10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4332efdd8713ed3c8a6d6738b0956673f1551c6"
      },
      "cell_type": "code",
      "source": "#Modeling Steps.\n\nrandom_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state = random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),\n                                      random_state=random_state,learning_rate =0.1))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state=random_state))\n\n\n\ncv_results = []\n\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y_train, scoring=\"accuracy\", cv = kfold, n_jobs=-1))\n    \ncv_means = []\ncv_std = []\n\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_res = pd.DataFrame({\"CrossValMeans\": cv_means, \"CrossValErrors\": cv_std, \n                       \"Algorithm\": [\"SVC\", \"RandomForestClassifier\", \"AdaBoostClassifier\",\n                                     \"Decision Tree Classifier\", \"Extra Trees\" ,\"Gradient Boosting\", \n                                     \"K Nearest Neighbors\", \"Logistic regression\"]})\n\ncv_res = cv_res.sort_values(by = \"CrossValMeans\", ascending=False)\ncv_res",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71854a12ad95871cf7e8b2ed1afb8ecfe0b2f602"
      },
      "cell_type": "code",
      "source": "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99a587787b34ef4214439973fb841f22d31683eb"
      },
      "cell_type": "code",
      "source": "#Predictions on test data\ngbc_clf = GradientBoostingClassifier(random_state=random_state)\ngbc_clf.fit(X_train, y_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56d18d263d6979029a14e104d5c44a12197a782e"
      },
      "cell_type": "code",
      "source": "Y_pred = gbc_clf.predict(test_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9887a02b6b95430d94d6402b98ee49bc18b676a"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame({\n        \"PassengerId\": IDtest,\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv('Titanic_Prediction_v3.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69949f30f72df4121a5efe6d8f71551dc2f28cc5"
      },
      "cell_type": "markdown",
      "source": "# To do in next Version"
    },
    {
      "metadata": {
        "_uuid": "ab88413e8eb5276c576a557e1ffb6d8105f6f8ee"
      },
      "cell_type": "markdown",
      "source": "*  More feature engineering\n*  Hyper parameter Tuning\n*  Ensemble Modeling\n*  Learning Curve Graphs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}